{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ¤– Naive Bayes from Scratch\n",
        "\n",
        "## ðŸŽ¯ This is a CRITICAL Pinterest Interview Topic!\n",
        "\n",
        "You WILL be asked to implement Naive Bayes in the CodeSignal assessment.\n",
        "\n",
        "## What You'll Learn\n",
        "1. **Bayes' Theorem** - The foundation\n",
        "2. **The \"Naive\" Assumption** - Why it's called naive\n",
        "3. **Implementation** - Step by step from scratch\n",
        "4. **Variants** - Gaussian, Multinomial, Bernoulli\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Š Interview Format\n",
        "- CodeSignal: Implement Naive Bayes in Python (~15 minutes)\n",
        "- Technical Interview: Explain the algorithm and its assumptions\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”‘ Key Formula to Memorize\n",
        "\n",
        "**Bayes' Theorem:**\n",
        "$$P(class|features) = \\frac{P(features|class) \\cdot P(class)}{P(features)}$$\n",
        "\n",
        "**Simplified (for classification):**\n",
        "$$P(class|features) \\propto P(class) \\cdot P(features|class)$$\n",
        "\n",
        "**With Naive Independence:**\n",
        "$$P(class|features) \\propto P(class) \\cdot \\prod_{i} P(feature_i|class)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "print(\"âœ… Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: Understanding Through a Simple Example\n",
        "\n",
        "## ðŸŒ§ï¸ Weather Prediction Example\n",
        "\n",
        "Let's predict if someone will **play tennis** based on weather.\n",
        "\n",
        "| Outlook  | Temp | Humidity | Windy | Play? |\n",
        "|----------|------|----------|-------|-------|\n",
        "| Sunny    | Hot  | High     | No    | No    |\n",
        "| Sunny    | Hot  | High     | Yes   | No    |\n",
        "| Overcast | Hot  | High     | No    | Yes   |\n",
        "| Rainy    | Mild | High     | No    | Yes   |\n",
        "| Rainy    | Cool | Normal   | No    | Yes   |\n",
        "| Rainy    | Cool | Normal   | Yes   | No    |\n",
        "| Overcast | Cool | Normal   | Yes   | Yes   |\n",
        "| Sunny    | Mild | High     | No    | No    |\n",
        "| Sunny    | Cool | Normal   | No    | Yes   |\n",
        "| Rainy    | Mild | Normal   | No    | Yes   |\n",
        "\n",
        "**Question:** Will someone play tennis if it's Sunny, Cool, High humidity, and Windy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸŽ® Interactive: Step-by-Step Naive Bayes Calculation\n",
        "\n",
        "# Our training data\n",
        "data = [\n",
        "    ('Sunny', 'Hot', 'High', 'No', 'No'),\n",
        "    ('Sunny', 'Hot', 'High', 'Yes', 'No'),\n",
        "    ('Overcast', 'Hot', 'High', 'No', 'Yes'),\n",
        "    ('Rainy', 'Mild', 'High', 'No', 'Yes'),\n",
        "    ('Rainy', 'Cool', 'Normal', 'No', 'Yes'),\n",
        "    ('Rainy', 'Cool', 'Normal', 'Yes', 'No'),\n",
        "    ('Overcast', 'Cool', 'Normal', 'Yes', 'Yes'),\n",
        "    ('Sunny', 'Mild', 'High', 'No', 'No'),\n",
        "    ('Sunny', 'Cool', 'Normal', 'No', 'Yes'),\n",
        "    ('Rainy', 'Mild', 'Normal', 'No', 'Yes'),\n",
        "]\n",
        "\n",
        "# Extract features and labels\n",
        "outlook = [d[0] for d in data]\n",
        "temp = [d[1] for d in data]\n",
        "humidity = [d[2] for d in data]\n",
        "windy = [d[3] for d in data]\n",
        "play = [d[4] for d in data]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"STEP 1: Calculate PRIOR Probabilities P(class)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Count outcomes\n",
        "play_yes = play.count('Yes')\n",
        "play_no = play.count('No')\n",
        "total = len(play)\n",
        "\n",
        "p_yes = play_yes / total\n",
        "p_no = play_no / total\n",
        "\n",
        "print(f\"P(Play=Yes) = {play_yes}/{total} = {p_yes:.3f}\")\n",
        "print(f\"P(Play=No)  = {play_no}/{total} = {p_no:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"STEP 2: Calculate LIKELIHOOD Probabilities P(feature|class)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Helper to calculate P(feature=value | class=label)\n",
        "def likelihood(feature_list, class_list, feat_value, class_value):\n",
        "    \"\"\"Calculate P(feature=value | class=class_value)\"\"\"\n",
        "    # Filter to samples where class = class_value\n",
        "    class_indices = [i for i, c in enumerate(class_list) if c == class_value]\n",
        "    class_count = len(class_indices)\n",
        "    \n",
        "    # Count how many have feature = feat_value\n",
        "    match_count = sum(1 for i in class_indices if feature_list[i] == feat_value)\n",
        "    \n",
        "    return match_count / class_count, match_count, class_count\n",
        "\n",
        "# Calculate likelihoods for our test case: Sunny, Cool, High, Windy=Yes\n",
        "print(\"\\nFor test case: Outlook=Sunny, Temp=Cool, Humidity=High, Windy=Yes\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# P(Outlook=Sunny | Play=Yes)\n",
        "p, num, den = likelihood(outlook, play, 'Sunny', 'Yes')\n",
        "print(f\"P(Outlook=Sunny | Play=Yes) = {num}/{den} = {p:.3f}\")\n",
        "\n",
        "# P(Outlook=Sunny | Play=No)\n",
        "p, num, den = likelihood(outlook, play, 'Sunny', 'No')\n",
        "print(f\"P(Outlook=Sunny | Play=No)  = {num}/{den} = {p:.3f}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# P(Temp=Cool | Play=Yes)\n",
        "p, num, den = likelihood(temp, play, 'Cool', 'Yes')\n",
        "print(f\"P(Temp=Cool | Play=Yes) = {num}/{den} = {p:.3f}\")\n",
        "\n",
        "# P(Temp=Cool | Play=No)\n",
        "p, num, den = likelihood(temp, play, 'Cool', 'No')\n",
        "print(f\"P(Temp=Cool | Play=No)  = {num}/{den} = {p:.3f}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# P(Humidity=High | Play=Yes)\n",
        "p, num, den = likelihood(humidity, play, 'High', 'Yes')\n",
        "print(f\"P(Humidity=High | Play=Yes) = {num}/{den} = {p:.3f}\")\n",
        "\n",
        "# P(Humidity=High | Play=No)\n",
        "p, num, den = likelihood(humidity, play, 'High', 'No')\n",
        "print(f\"P(Humidity=High | Play=No)  = {num}/{den} = {p:.3f}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# P(Windy=Yes | Play=Yes)\n",
        "p, num, den = likelihood(windy, play, 'Yes', 'Yes')\n",
        "print(f\"P(Windy=Yes | Play=Yes) = {num}/{den} = {p:.3f}\")\n",
        "\n",
        "# P(Windy=Yes | Play=No)\n",
        "p, num, den = likelihood(windy, play, 'Yes', 'No')\n",
        "print(f\"P(Windy=Yes | Play=No)  = {num}/{den} = {p:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"STEP 3: Combine with NAIVE INDEPENDENCE Assumption\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "print(\"The NAIVE part: Assume features are independent given the class!\")\n",
        "print()\n",
        "print(\"P(Play=Yes | features) âˆ P(Yes) Ã— P(Sunny|Yes) Ã— P(Cool|Yes) Ã— P(High|Yes) Ã— P(Windy|Yes)\")\n",
        "print(\"P(Play=No  | features) âˆ P(No)  Ã— P(Sunny|No)  Ã— P(Cool|No)  Ã— P(High|No)  Ã— P(Windy|No)\")\n",
        "print()\n",
        "\n",
        "# Calculate posterior for Play=Yes\n",
        "p_sunny_yes, _, _ = likelihood(outlook, play, 'Sunny', 'Yes')\n",
        "p_cool_yes, _, _ = likelihood(temp, play, 'Cool', 'Yes')\n",
        "p_high_yes, _, _ = likelihood(humidity, play, 'High', 'Yes')\n",
        "p_windy_yes, _, _ = likelihood(windy, play, 'Yes', 'Yes')\n",
        "\n",
        "posterior_yes = p_yes * p_sunny_yes * p_cool_yes * p_high_yes * p_windy_yes\n",
        "print(f\"P(Yes|features) âˆ {p_yes:.2f} Ã— {p_sunny_yes:.2f} Ã— {p_cool_yes:.2f} Ã— {p_high_yes:.2f} Ã— {p_windy_yes:.2f}\")\n",
        "print(f\"               = {posterior_yes:.6f}\")\n",
        "print()\n",
        "\n",
        "# Calculate posterior for Play=No\n",
        "p_sunny_no, _, _ = likelihood(outlook, play, 'Sunny', 'No')\n",
        "p_cool_no, _, _ = likelihood(temp, play, 'Cool', 'No')\n",
        "p_high_no, _, _ = likelihood(humidity, play, 'High', 'No')\n",
        "p_windy_no, _, _ = likelihood(windy, play, 'Yes', 'No')\n",
        "\n",
        "posterior_no = p_no * p_sunny_no * p_cool_no * p_high_no * p_windy_no\n",
        "print(f\"P(No|features)  âˆ {p_no:.2f} Ã— {p_sunny_no:.2f} Ã— {p_cool_no:.2f} Ã— {p_high_no:.2f} Ã— {p_windy_no:.2f}\")\n",
        "print(f\"               = {posterior_no:.6f}\")\n",
        "\n",
        "print()\n",
        "print(\"=\"*60)\n",
        "print(\"STEP 4: Make Prediction (choose higher probability)\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "if posterior_yes > posterior_no:\n",
        "    print(f\"ðŸŽ¾ PREDICTION: Play = YES (posterior {posterior_yes:.6f} > {posterior_no:.6f})\")\n",
        "else:\n",
        "    print(f\"ðŸŒ§ï¸ PREDICTION: Play = NO (posterior {posterior_no:.6f} > {posterior_yes:.6f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Gaussian Naive Bayes (For Continuous Features)\n",
        "\n",
        "## ðŸŽ¯ This is What You'll Implement in the Interview!\n",
        "\n",
        "For continuous features (like height, weight, temperature), we can't count occurrences.\n",
        "Instead, we assume features follow a **Gaussian (normal) distribution**.\n",
        "\n",
        "## The Gaussian PDF Formula\n",
        "\n",
        "$$P(x | class) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$$\n",
        "\n",
        "Where:\n",
        "- Î¼ (mu) = mean of feature for this class\n",
        "- ÏƒÂ² (sigma squared) = variance of feature for this class\n",
        "\n",
        "## Implementation Steps\n",
        "1. **Fit:** For each class, calculate mean and variance of each feature\n",
        "2. **Predict:** Use Gaussian PDF to get P(feature|class), multiply together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ“– COMPLETE GAUSSIAN NAIVE BAYES IMPLEMENTATION\n",
        "# This is what you'd write in the interview!\n",
        "\n",
        "class GaussianNaiveBayes:\n",
        "    \"\"\"\n",
        "    Gaussian Naive Bayes classifier for continuous features.\n",
        "    \n",
        "    This is the INTERVIEW VERSION - clean and complete.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.classes = None\n",
        "        self.means = {}      # {class: [mean_feat1, mean_feat2, ...]}\n",
        "        self.variances = {}  # {class: [var_feat1, var_feat2, ...]}\n",
        "        self.priors = {}     # {class: prior_probability}\n",
        "    \n",
        "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
        "        \"\"\"\n",
        "        Fit the model: calculate mean, variance, and prior for each class.\n",
        "        \n",
        "        Args:\n",
        "            X: Training features (n_samples, n_features)\n",
        "            y: Training labels (n_samples,)\n",
        "        \"\"\"\n",
        "        self.classes = np.unique(y)\n",
        "        n_samples = len(y)\n",
        "        \n",
        "        for c in self.classes:\n",
        "            # Get all samples of this class\n",
        "            X_c = X[y == c]\n",
        "            \n",
        "            # Calculate statistics\n",
        "            self.means[c] = np.mean(X_c, axis=0)\n",
        "            self.variances[c] = np.var(X_c, axis=0) + 1e-9  # Add epsilon for stability\n",
        "            self.priors[c] = len(X_c) / n_samples\n",
        "            \n",
        "            print(f\"Class {c}:\")\n",
        "            print(f\"  Prior: {self.priors[c]:.3f}\")\n",
        "            print(f\"  Means: {self.means[c]}\")\n",
        "            print(f\"  Variances: {self.variances[c]}\")\n",
        "            print()\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def _gaussian_pdf(self, x, mean, var):\n",
        "        \"\"\"\n",
        "        Calculate Gaussian probability density.\n",
        "        \n",
        "        P(x|Î¼,ÏƒÂ²) = (1/âˆš(2Ï€ÏƒÂ²)) Ã— exp(-(x-Î¼)Â²/(2ÏƒÂ²))\n",
        "        \"\"\"\n",
        "        coef = 1.0 / np.sqrt(2 * np.pi * var)\n",
        "        exponent = np.exp(-(x - mean) ** 2 / (2 * var))\n",
        "        return coef * exponent\n",
        "    \n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Predict class labels for samples.\"\"\"\n",
        "        return np.array([self._predict_single(x) for x in X])\n",
        "    \n",
        "    def _predict_single(self, x):\n",
        "        \"\"\"Predict class for a single sample.\"\"\"\n",
        "        posteriors = {}\n",
        "        \n",
        "        for c in self.classes:\n",
        "            # Start with log prior\n",
        "            posterior = np.log(self.priors[c])\n",
        "            \n",
        "            # Add log likelihood for each feature\n",
        "            for i in range(len(x)):\n",
        "                pdf = self._gaussian_pdf(x[i], self.means[c][i], self.variances[c][i])\n",
        "                posterior += np.log(pdf + 1e-300)  # Avoid log(0)\n",
        "            \n",
        "            posteriors[c] = posterior\n",
        "        \n",
        "        # Return class with highest posterior\n",
        "        return max(posteriors, key=posteriors.get)\n",
        "\n",
        "# Create demo data: two classes with different means\n",
        "print(\"=\"*60)\n",
        "print(\"GAUSSIAN NAIVE BAYES DEMO\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Class 0: centered at (0, 0)\n",
        "X_class0 = np.random.randn(50, 2) + np.array([0, 0])\n",
        "# Class 1: centered at (3, 3)\n",
        "X_class1 = np.random.randn(50, 2) + np.array([3, 3])\n",
        "\n",
        "X_train = np.vstack([X_class0, X_class1])\n",
        "y_train = np.array([0] * 50 + [1] * 50)\n",
        "\n",
        "# Fit the model\n",
        "print(\"Fitting model...\\n\")\n",
        "clf = GaussianNaiveBayes()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Test predictions\n",
        "print(\"=\"*60)\n",
        "print(\"Testing predictions:\")\n",
        "print(\"=\"*60)\n",
        "test_samples = np.array([\n",
        "    [0, 0],      # Should be class 0\n",
        "    [3, 3],      # Should be class 1\n",
        "    [1.5, 1.5],  # Boundary - could be either\n",
        "])\n",
        "\n",
        "for sample in test_samples:\n",
        "    pred = clf._predict_single(sample)\n",
        "    print(f\"Sample {sample} â†’ Predicted class: {pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ“‹ INTERVIEW CHEAT SHEET: Naive Bayes\n",
        "\n",
        "## What to Say in the Interview\n",
        "\n",
        "### \"What is Naive Bayes?\"\n",
        "> \"Naive Bayes is a probabilistic classifier based on Bayes' theorem. It's called 'naive' because it assumes features are independent given the class, which simplifies the computation significantly.\"\n",
        "\n",
        "### \"What is the naive assumption?\"\n",
        "> \"The naive assumption is that features are conditionally independent given the class label. This means P(features|class) = P(f1|class) Ã— P(f2|class) Ã— ... This is rarely true in practice but works surprisingly well.\"\n",
        "\n",
        "### \"When would you use Naive Bayes?\"\n",
        "> \"Naive Bayes works well for:\n",
        "> - Text classification (spam detection, sentiment analysis)\n",
        "> - When you have limited training data\n",
        "> - When features are actually somewhat independent\n",
        "> - As a baseline model that's fast and interpretable\"\n",
        "\n",
        "### \"What are the variants?\"\n",
        "> \"Three main variants:\n",
        "> - **Gaussian**: For continuous features (assumes normal distribution)\n",
        "> - **Multinomial**: For count data (like word counts in text)\n",
        "> - **Bernoulli**: For binary features (like word presence/absence)\"\n",
        "\n",
        "### \"What's Laplace smoothing?\"\n",
        "> \"Laplace smoothing (add-one smoothing) prevents zero probabilities when a feature value wasn't seen in training. We add a small count (usually 1) to avoid multiplying by zero.\"\n",
        "\n",
        "## Implementation Checklist\n",
        "1. âœ… Calculate prior probabilities P(class)\n",
        "2. âœ… Calculate likelihood P(feature|class) for each feature\n",
        "3. âœ… Use log probabilities to avoid underflow\n",
        "4. âœ… Add smoothing/epsilon for numerical stability\n",
        "5. âœ… Predict by choosing class with highest posterior\n",
        "\n",
        "## Time Complexity\n",
        "- **Training:** O(n Ã— d) - one pass through data\n",
        "- **Prediction:** O(c Ã— d) - for each class and feature\n",
        "\n",
        "Where n=samples, d=features, c=classes"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
